"""
SanchÃ©z Meza Ariadna Osiris 
SIAFI | PropedÃ©utico TÃ©cnico 2025-2
Proyecto

"""
#Ejercicio 2.
#ClasificaciÃ³n de frases segÃºn emojis. k-Nearest Neighbo

import numpy as np
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import TfidfVectorizer #la usamos para convertir los vectores
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier #para utiliar el clasificador k-NN
from sklearn.decomposition import PCA

#Creamos un diccionario con frases randm y emojs que podrÃ­an representarlas
data = [
    {"frase": "Estoy muy feliz hoy", "emoji": "ğŸ˜Š"},
    {"frase": "El dÃ­a esta bonito", "emoji": "ğŸŒ"},
    {"frase": "Quisiera jugar D:", "emoji": "ğŸ˜¢"},
    {"frase": "Tengo cÃ³licos", "emoji": "ğŸ˜"},
    {"frase": "Eso brilla demasiado", "emoji": "ğŸ¤©"},
    {"frase": "Es muy picante", "emoji": "ğŸ˜¡"},
    {"frase": "Por supuesto que sÃ­", "emoji": "ğŸ˜Œ"},
    {"frase": "Se me olvido", "emoji": "ğŸ˜°"},
    {"frase": "Que lindoooo", "emoji": "ğŸ˜"},
    {"frase": "Ni modo", "emoji": "ğŸ˜•"},
    {"frase": "Me dio mucha risa", "emoji": "ğŸ˜†"},
    {"frase": "Ay!", "emoji": "ğŸ˜…"},
    {"frase": "Estoy llorando de la risa", "emoji": "ğŸ¤£"},
    {"frase": "Gracias!", "emoji": "ğŸ˜Š"},
    {"frase": "Obvio", "emoji": "ğŸ˜‡"},
    {"frase": "Fok", "emoji": "ğŸ™‚"},
    {"frase": "No  puede ser", "emoji": "ğŸ™ƒ"}
]
frases = [entry["frase"] for entry in data]
emojis = [entry["emoji"] for entry in data]

vector= TfidfVectorizer() #las frases las  convertimos a vectores  para  entrenar y predecir
X = vector.fit_transform(frases).toarray()

#aqui empezamos a dividir  en los datos prueba con un  30% y los de entremiento con un 70% es decir el 70 se utiliza para entrenar los datos y el 30 para despues del entrenamiento
X_train, X_test, y_train, y_test = train_test_split(X, emojis, test_size=0.3, random_state=42)#entrena y evalua

knn = KNeighborsClassifier(n_neighbors=3)# Utilizamos el kNN para observar el mayor numero de vecinos
knn.fit(X_train, y_train)#Cuando  lo igualamos a 3 queremos decir que tomamos los 3 vectores mÃ¡s cercanos
#Ahora comenzamos a predecir con x_test
y_pred = knn.predict(X_test)
#A partir de  aquÃ­ iteramos  sobre las predicciones
#comparando las reales con las de prediccion  
print("Resultados:") 
correctas = []
incorrectas = []
for i, (frase, real, pred) in enumerate(zip(frases, y_test, y_pred)):
    if real == pred:
        correctas.append((frase, real, pred))
    else:
        incorrectas.append((frase, real, pred))

print("Predicciones que son correctas:")
for frase, real, pred in correctas:
    print(f"Frase: {frase} / Dato real: {real} / PredicciÃ³n: {pred}")

print("\nPredicciones que son incorrectas:")
for frase, real, pred in incorrectas:
    print(f"Frase: {frase} / Dato real: {real} / PredicciÃ³n: {pred}")


fig, ax = plt.subplots()# Vamos viendo los  resultados en una grÃ¡fica para obsevar las rediccioes y elacercamiento que exise
colors = {"ğŸ˜Š": "blue", "ğŸŒ": "yellow", "ğŸ˜¢": "purple", "ğŸ˜": "red", "ğŸ¤©": "green",#AdemÃ¡s les asigne colores porque no  se me ocurrio de otra forma identificarlos
          "ğŸ˜¡": "orange", "ğŸ˜Œ": "cyan", "ğŸ˜°": "pink", "ğŸ˜": "magenta", "ğŸ˜•": "brown",
          "ğŸ˜†": "lime", "ğŸ˜…": "gray", "ğŸ¤£": "teal", "ğŸ˜‡": "olive", "ğŸ™‚": "navy",
          "ğŸ™ƒ": "gold"}

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

for i, (frase, emoji) in enumerate(zip(frases, emojis)):#Vamos itrando para cada frase y emoji 
    ax.scatter(X_pca[i, 0], X_pca[i, 1], color=colors[emoji], #la etiqueta es el emoji
               label=emoji if emoji not in ax.get_legend_handles_labels()[1] else "")#donde aparece cada frase 

#Ahora las predicciones para los datos de prueba que se representan con una cruz
X_test_pca = pca.transform(X_test)  # Reducir dimensiones de X_test con PCA
for i, (x, y, pred) in enumerate(zip(X_test_pca[:, 0], X_test_pca[:, 1], y_pred)):
    ax.scatter(x, y, color="black", marker="x", s=100, label="PredicciÃ³n" if "PredicciÃ³n" not in ax.get_legend_handles_labels()[1] else "")
    ax.annotate(pred, (x, y), textcoords="offset points", xytext=(5, 5), ha='center', fontsize=10)

ax.set_title("Frases y Emojis")#Graficamos 
ax.set_xlabel("Parte 1")
ax.set_ylabel("Parte 2")
plt.tight_layout()
plt.show()
#podemos ver que los emojis en la grÃ¡fica representan los datos reales es decir las frases
#Por otro lado las cruces representan las predicciones para los datos prueba
#Cada emoji  estarepresentado con un color y cuando vemos una cruz encima quiere decir que la predicciÃ³n fue correcta
#En cambio si no hay cruz encima quiere decir que la predicciÃ³n fue incorrecta
#En la terminal  podemos verque en las predicciones correctas no aparecen como tal, estas se muestran en lagrÃ¡fica "fisicamente"
#Notando que en la parte de arriba se muestr el emoji de la predicciÃ³n es decir la cruz
#Cuando  vemos las predicciones incorrectas son las que no aparecen  por completo en la grÃ¡fica
#Tambien se observa que en  las predicciones tienden a estar con el emoji de la frase de "ni  modo"
#Concluyo que se debe a que existe un sesgo hacia esa frase porque es similar  a  las frases de prueba o porqu es "similar" a los demÃ¡s emojis
#Considero que con mÃ¡s frases las predcciones serÃ¡n mÃ¡s varantes y no se sesgarÃ¡n tanto